{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7U2_OTs6kNQv"
      },
      "source": [
        "# Workflow for models training\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBt9EyDnkNQx"
      },
      "source": [
        "## Libraries and read in cleaned data\n",
        "\n",
        "Data cleaning (done by Yvonne) and following steps were taken:\n",
        "- removing rows with nan in RT\n",
        "- removing rows with nan in concentration\n",
        "- removing calibration graphs with only 1 or 2 calibration points\n",
        "\n",
        "Data set contains 3860 rows and no nan values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "uKLeYc9YGqOJ",
        "outputId": "504663ca-5ac0-40b9-a56f-fe8d21d29d37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3850 entries, 0 to 3849\n",
            "Data columns (total 14 columns):\n",
            " #   Column         Non-Null Count  Dtype  \n",
            "---  ------         --------------  -----  \n",
            " 0   lab            3850 non-null   object \n",
            " 1   compound       3850 non-null   object \n",
            " 2   sample_type    3850 non-null   object \n",
            " 3   RT             3850 non-null   float64\n",
            " 4   sample         3850 non-null   object \n",
            " 5   peak_area      3850 non-null   float64\n",
            " 6   note           3850 non-null   object \n",
            " 7   c_real_M       3850 non-null   float64\n",
            " 8   rf             3850 non-null   float64\n",
            " 9   rf_error       3850 non-null   float64\n",
            " 10  slope          3850 non-null   float64\n",
            " 11  intercept      3850 non-null   float64\n",
            " 12  residuals      3850 non-null   float64\n",
            " 13  abs_residuals  3850 non-null   float64\n",
            "dtypes: float64(9), object(5)\n",
            "memory usage: 421.2+ KB\n"
          ]
        }
      ],
      "source": [
        "# libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from plotnine import *\n",
        "\n",
        "# data\n",
        "file_path = \"../0_data/data_ready_addfeatures_231128.csv\"\n",
        "# file_path = \"C:/Users/yvkr1259/Documents/data_ready_addfeatures_231122.csv\"\n",
        "\n",
        "df_calibrations = pd.read_csv(file_path)\n",
        "# remove all the normaized columns \n",
        "drop_columns = ['abs_residuals_norm1', 'abs_residuals_norm2','c_real_M_norm1','c_real_M_norm2','peak_area_norm1',\n",
        "'peak_area_norm2','residuals_norm1','residuals_norm2','rf_error_norm1','rf_error_norm2','rf_norm1','rf_norm2']\n",
        "\n",
        "df_calibrations = df_calibrations.drop(drop_columns, axis=1)\n",
        "df_calibrations.info()\n",
        "\n",
        "## load data to google colab\n",
        "#from google.colab import files\n",
        "#uploaded = files.upload()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7o6zGXQpJho",
        "outputId": "fef4362f-0576-4306-85cf-bd0247ebdc50"
      },
      "outputs": [],
      "source": [
        "#file_path = \"data_ready_addfeatures_231122.csv\"\n",
        "#df_calibrations = pd.read_csv(file_path)\n",
        "#df_calibrations.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Select features and data splitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data splitting should consider that points for each compound per lab belong together. Therefore an individual id for each compound lab pair is introduced. Splitting is then performed based on the id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_calibrations['id'] = df_calibrations['lab'] + '_' + df_calibrations['compound']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PvHrHLsyS4Xz",
        "outputId": "e928f27b-fff0-4d62-f718-6e3da88283b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(3084, 14)\n",
            "(3084, 1)\n",
            "(766, 14)\n",
            "(766, 1)\n"
          ]
        }
      ],
      "source": [
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split dataset into training set and test set based on id \n",
        "unique_ids = df_calibrations['id'].unique()\n",
        "np.random.seed(123)\n",
        "train_ids, test_ids = train_test_split(unique_ids, test_size=0.2, random_state=42) # 80% training and 20% test\n",
        "\n",
        "\n",
        "df_train = df_calibrations[df_calibrations['id'].isin(train_ids)]\n",
        "df_test = df_calibrations[df_calibrations['id'].isin(test_ids)]\n",
        "\n",
        "# Split dataset into features and target variable\n",
        "X_train =  df_train.drop('note', axis=1)\n",
        "y_train = df_train[['note']]\n",
        "X_test = df_test.drop('note', axis=1)\n",
        "y_test = df_test[['note']]\n",
        "\n",
        "\n",
        "print(X_train.shape) \n",
        "print(y_train.shape) \n",
        "print(X_test.shape) \n",
        "print(y_test.shape) \n",
        "\n",
        "# (3093, 14)\n",
        "# (3093, 1)\n",
        "# (767, 14)\n",
        "# (767, 1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In general we decided to try out two different normalisation strategies:\n",
        "\n",
        "**norm1**\n",
        "$$\n",
        "\\text{norm1} = \\frac{\\text{x}}{\\max(\\text{x})}\n",
        "$$\n",
        "\n",
        "**norm2**\n",
        "$$\n",
        "\\text{norm2} = \\frac{\\text{x} - \\min(\\text{x})}{\\max(\\text{x}) - \\min(\\text{x})}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### normalization strategy 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# train set \n",
        "X_train['peak_area_norm1'] = X_train.groupby(['lab', 'compound'])['peak_area'].transform(lambda x: x / x.max())\n",
        "X_train['c_real_M_norm1'] = X_train.groupby(['lab', 'compound'])['c_real_M'].transform(lambda x: x / x.max())\n",
        "X_train['rf_norm1'] = X_train.groupby(['lab', 'compound'])['rf'].transform(lambda x: x / x.max())\n",
        "X_train['rf_error_norm1'] = X_train.groupby(['lab', 'compound'])['rf_error'].transform(lambda x: x / x.max())\n",
        "X_train['residuals_norm1'] = X_train.groupby(['lab', 'compound'])['residuals'].transform(lambda x: x / x.max())\n",
        "X_train['abs_residuals_norm1'] = X_train.groupby(['lab', 'compound'])['abs_residuals'].transform(lambda x: x / x.max())\n",
        "\n",
        "# test set \n",
        "X_test['peak_area_norm1'] = X_test.groupby(['lab', 'compound'])['peak_area'].transform(lambda x: x / x.max())\n",
        "X_test['c_real_M_norm1'] = X_test.groupby(['lab', 'compound'])['c_real_M'].transform(lambda x: x / x.max())\n",
        "X_test['rf_norm1'] = X_test.groupby(['lab', 'compound'])['rf'].transform(lambda x: x / x.max())\n",
        "X_test['rf_error_norm1'] = X_test.groupby(['lab', 'compound'])['rf_error'].transform(lambda x: x / x.max())\n",
        "X_test['residuals_norm1'] = X_test.groupby(['lab', 'compound'])['residuals'].transform(lambda x: x / x.max())\n",
        "X_test['abs_residuals_norm1'] = X_test.groupby(['lab', 'compound'])['abs_residuals'].transform(lambda x: x / x.max())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### normalization strategy 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "columns_to_scale = ['peak_area', 'c_real_M', 'rf', 'rf_error', 'residuals', 'abs_residuals']\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "def scale_columns(group):\n",
        "    for col in columns_to_scale:\n",
        "        group[f'{col}_norm2'] = scaler.fit_transform(group[[col]])\n",
        "    return group\n",
        "\n",
        "\n",
        "X_train = X_train.groupby(['lab', 'compound'], group_keys = False).apply(scale_columns) # group_keys = False to not include group keys in the resulting df\n",
        "X_test = X_test.groupby(['lab', 'compound'], group_keys = False).apply(scale_columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "WliS5ii_PVS3"
      },
      "outputs": [],
      "source": [
        "## Decide on features for modelling\n",
        "#features = ['peak_area','c_real_M']\n",
        "#features = ['RT','peak_area','c_real_M']\n",
        "#features = ['RT','peak_area','c_real_M', 'rf', 'rf_error']\n",
        "#features = ['RT','peak_area','c_real_M', 'rf', 'rf_error', 'slope', 'intercept', 'residuals', 'abs_residuals']\n",
        "features = ['RT','peak_area_norm1','c_real_M_norm1', 'rf_norm1', 'rf_error_norm1', 'slope', 'intercept', 'residuals_norm1', 'abs_residuals_norm1'] # best features\n",
        "#eatures = ['RT','peak_area_norm2','c_real_M_norm2', 'rf_norm2', 'rf_error_norm2', 'slope', 'intercept', 'residuals_norm2', 'abs_residuals_norm2']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train = X_train[features]\n",
        "X_test = X_test[features]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Converting labels to 0 (linear) and 1 (non-linear) \n",
        "y_train = y_train.replace({'linear': 0, 'non-linear': 1})\n",
        "y_test = y_test.replace({'linear': 0, 'non-linear': 1})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Modeling "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Libraries\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score, RandomizedSearchCV\n",
        "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, confusion_matrix\n",
        "from xgboost import XGBClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert the labels (y_train and y_test) into 1D arrays\n",
        "y_train_1D = y_train.values.ravel()\n",
        "y_test_1D = y_test.values.ravel()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
            "AUC Score: 0.909880834160874\n",
            "Validation Accuracy: 0.8263707571801566\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.84      0.84       424\n",
            "           1       0.80      0.81      0.81       342\n",
            "\n",
            "    accuracy                           0.83       766\n",
            "   macro avg       0.82      0.82      0.82       766\n",
            "weighted avg       0.83      0.83      0.83       766\n",
            "\n",
            "Confusion Matrix:\n",
            " [[356  68]\n",
            " [ 65 277]]\n",
            "Best Hyperparameters:\n",
            " {'n_estimators': 50, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 10, 'criterion': 'entropy', 'bootstrap': True}\n"
          ]
        }
      ],
      "source": [
        "# Hyperparameter search\n",
        "param_distributions = {\n",
        "    'n_estimators': [50, 100, 300, 500],\n",
        "    'max_depth': [None, 10, 30, 50],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'bootstrap': [True, False],\n",
        "    'max_features': ['sqrt', 'log2'],\n",
        "    'criterion': ['gini', 'entropy']\n",
        "}\n",
        "\n",
        "# Initialize the base model\n",
        "rf = RandomForestClassifier(random_state = 1)\n",
        "\n",
        "# Set up the RandomizedSearchCV object\n",
        "rf_random_search = RandomizedSearchCV(\n",
        "    estimator = rf,\n",
        "    param_distributions = param_distributions,\n",
        "    n_iter = 100,\n",
        "    cv = 5, \n",
        "    verbose = 2,\n",
        "    random_state = 1,\n",
        "    n_jobs = -1 # uses all cores!!\n",
        ")\n",
        "\n",
        "# Fit the RandomizedSearchCV object to the training data\n",
        "rf_random_search.fit(X_train, y_train_1D)\n",
        "\n",
        "# Get the best estimator\n",
        "best_rf = rf_random_search.best_estimator_\n",
        "\n",
        "# Fit the model with the best hyperparameters on the full oversampled training data\n",
        "best_rf.fit(X_train, y_train_1D)\n",
        "\n",
        "# Predicting probabilities on the validation set for AUC calculation\n",
        "prob_predictions = best_rf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate AUC\n",
        "auc_score = roc_auc_score(y_test_1D, prob_predictions)\n",
        "print(f\"AUC Score: {auc_score}\")\n",
        "\n",
        "# Predicting class labels (for accuracy, confusion matrix, etc.)\n",
        "class_predictions = best_rf.predict(X_test)\n",
        "\n",
        "# Evaluating the model on the validation set\n",
        "val_accuracy = accuracy_score(y_test_1D, class_predictions)\n",
        "print(f\"Validation Accuracy: {val_accuracy}\")\n",
        "\n",
        "# Detailed classification report\n",
        "print(\"Classification Report:\\n\", classification_report(y_test_1D, class_predictions))\n",
        "\n",
        "# Confusion Matrix\n",
        "conf_matrix = confusion_matrix(y_test_1D, class_predictions)\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(\"Best Hyperparameters:\\n\", rf_random_search.best_params_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Best Hyperparameters RF: </br>\n",
        " {'n_estimators': 50, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 10, 'criterion': 'entropy', 'bootstrap': True}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "               feature  importance\n",
            "3             rf_norm1    0.354963\n",
            "2       c_real_M_norm1    0.118938\n",
            "1      peak_area_norm1    0.118811\n",
            "4       rf_error_norm1    0.086824\n",
            "6            intercept    0.084462\n",
            "0                   RT    0.073340\n",
            "7      residuals_norm1    0.066759\n",
            "5                slope    0.049460\n",
            "8  abs_residuals_norm1    0.046443\n"
          ]
        }
      ],
      "source": [
        "# Get feature importances\n",
        "importances = best_rf.feature_importances_\n",
        "\n",
        "# Assuming X_train is a DataFrame with column names\n",
        "feature_names = X_train.columns\n",
        "\n",
        "# Create a DataFrame for feature importances\n",
        "feature_importances_df = pd.DataFrame({\n",
        "    'feature': feature_names,\n",
        "    'importance': importances\n",
        "})\n",
        "\n",
        "# Sort the DataFrame by importance\n",
        "feature_importances_df = feature_importances_df.sort_values(by='importance', ascending=False)\n",
        "\n",
        "# Display the feature importances\n",
        "print(feature_importances_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### XGBoost\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
            "Validation ROC AUC Score: 0.9073913163411674\n",
            "Validation Accuracy: 0.8185378590078329\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.83      0.84       424\n",
            "           1       0.80      0.80      0.80       342\n",
            "\n",
            "    accuracy                           0.82       766\n",
            "   macro avg       0.82      0.82      0.82       766\n",
            "weighted avg       0.82      0.82      0.82       766\n",
            "\n",
            "[[354  70]\n",
            " [ 69 273]]\n",
            "Best parameters:\n",
            " {'subsample': 0.7, 'n_estimators': 500, 'min_child_weight': 10, 'max_depth': 6, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n"
          ]
        }
      ],
      "source": [
        "# Define the hyperparameter grid to search\n",
        "param_grid = {\n",
        "    'max_depth': [3, 6, 10],\n",
        "    'min_child_weight': [1, 5, 10],\n",
        "    'subsample': [0.5, 0.7, 1.0],\n",
        "    'colsample_bytree': [0.5, 0.7, 1.0],\n",
        "    'n_estimators': [100, 200, 500],\n",
        "    'learning_rate': [0.01, 0.1, 0.2]\n",
        "}\n",
        "\n",
        "# Initialize the XGBClassifier\n",
        "xgb = XGBClassifier(use_label_encoder = False, eval_metric = 'logloss')\n",
        "\n",
        "# Set up RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator = xgb,\n",
        "    param_distributions = param_grid,\n",
        "    n_iter = 100, \n",
        "    scoring = 'roc_auc', \n",
        "    cv = 5,\n",
        "    verbose = 2,\n",
        "    random_state = 1,\n",
        "    n_jobs = -1  # Use all available cores\n",
        ")\n",
        "\n",
        "# Fit the RandomizedSearchCV object to the training data\n",
        "random_search.fit(X_train, y_train_1D)\n",
        "\n",
        "# Get the best estimator\n",
        "best_xgb = random_search.best_estimator_\n",
        "\n",
        "# Train the best estimator on the full training data\n",
        "best_xgb.fit(X_train, y_train_1D)\n",
        "\n",
        "# Predict probabilities on the validation set\n",
        "prob_predictions = best_xgb.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Compute the ROC AUC score\n",
        "val_roc_auc = roc_auc_score(y_test_1D, prob_predictions)\n",
        "print(f\"Validation ROC AUC Score: {val_roc_auc}\")\n",
        "\n",
        "# Predict on the validation set\n",
        "class_predictions = best_xgb.predict(X_test)\n",
        "\n",
        "# Compute accuracy\n",
        "val_accuracy = accuracy_score(y_test_1D, class_predictions)\n",
        "print(f\"Validation Accuracy: {val_accuracy}\")\n",
        "\n",
        "# Print classification report\n",
        "print(classification_report(y_test_1D, class_predictions))\n",
        "\n",
        "# Print confusion matrix\n",
        "print(confusion_matrix(y_test_1D, class_predictions))\n",
        "\n",
        "# Print the best parameters\n",
        "print(\"Best parameters:\\n\", random_search.best_params_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Best parameters XGBoost: </br>\n",
        " {'subsample': 0.7, 'n_estimators': 500, 'min_child_weight': 10, 'max_depth': 6, 'learning_rate': 0.01, 'colsample_bytree': 1.0}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "               feature  importance\n",
            "3             rf_norm1    0.327996\n",
            "1      peak_area_norm1    0.231957\n",
            "4       rf_error_norm1    0.085690\n",
            "2       c_real_M_norm1    0.080915\n",
            "0                   RT    0.062289\n",
            "6            intercept    0.061032\n",
            "7      residuals_norm1    0.053742\n",
            "5                slope    0.048285\n",
            "8  abs_residuals_norm1    0.048092\n"
          ]
        }
      ],
      "source": [
        "# Get feature importances\n",
        "importances = best_xgb.feature_importances_\n",
        "\n",
        "# Assuming X_train is a DataFrame with column names\n",
        "feature_names = X_train.columns\n",
        "\n",
        "# Create a DataFrame for feature importances\n",
        "feature_importances_df = pd.DataFrame({\n",
        "    'feature': feature_names,\n",
        "    'importance': importances\n",
        "})\n",
        "\n",
        "# Sort the DataFrame by importance\n",
        "feature_importances_df = feature_importances_df.sort_values(by='importance', ascending=False)\n",
        "\n",
        "# Display the feature importances\n",
        "print(feature_importances_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
